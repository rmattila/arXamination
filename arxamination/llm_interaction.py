from tqdm import tqdm
import json
from gpt4all import GPT4All
from typing import List, Dict
from .utils import load_config_file, tokens_to_chars


class BaseLLM:
    def __init__(self, config: Dict, verbose: bool):
        self.prompt_template = config["prompt_template"]
        self.summarize_template = config["summarize_template"]
        self.verbose = verbose

        # The 'context_length' attribute should be set in child classes for specific LLM implementations
        self.context_length = None

    def process_with_llm(self, text: str, question: str) -> str:
        """Process the text with the LLM for a given question and return a summary.

        We split the text into several chunks (that each will fit in the context length
        of the LLM). Then, once we have an answer based on each chunk, we summarize
        all the individual answers into a final answer for the whole text."""

        chunks = self.chunk_text(text)
        answers = []

        with tqdm(chunks, desc="Analyzing Chunks", leave=False) as pbar:
            for chunk_number, chunk in enumerate(pbar, 1):
                answer = self.get_answer(question, chunk)

                if self.verbose:
                    tqdm.write(f"Answer for Chunk {chunk_number}: {answer}\n")

                answers.append(answer)

        return self.summarize_answers(question, answers)

    def chunk_text(self, text: str) -> List[str]:
        """Split the text into chunks based on context_length.

        Args:
            text (str): The text to be split into chunks.

        Returns:
            List[str]: List of text chunks.
        """
        if self.context_length is None:
            raise ValueError("Context length has not been set.")

        # Calculate the character count per chunk while leaving space for the prompt
        chunk_char_count = tokens_to_chars(self.context_length) - len(
            self.prompt_template
        )

        # Apply a margin factor (e.g., the question should go into the prompt and we used averages above)
        chunk_char_count = int(chunk_char_count * 0.25)

        if self.verbose:
            print(f"Using a chunk size of {chunk_char_count} characters")

        chunks = [
            text[i : i + chunk_char_count]
            for i in range(0, len(text), chunk_char_count)
        ]
        return chunks

    def get_answer(self, question: str, chunk: str) -> str:
        """Get an answer from the LLM for a given question and chunk."""
        prompt = self.prompt_template.format(question=question, chunk=chunk)
        answer = self.get_LLM_response(prompt)
        return answer

    def summarize_answers(self, question: str, answers: List[str]) -> str:
        """Summarize all chunk-answers for a given question and return a summary."""
        answers_str = "\n".join(answers)
        prompt = self.summarize_template.format(question=question, answers=answers_str)
        summary = self.get_LLM_response(prompt)
        return summary

    def get_LLM_response(self, prompt: str) -> str:
        """Get a response from the LLM for a given prompt.

        This method should be implemented in subclasses for specific LLMs.

        Args:
            prompt (str): The prompt to be sent to the LLM.

        Returns:
            str: The response generated by the LLM.
        """
        raise NotImplementedError


class LocalLLM(BaseLLM):
    def __init__(self, config_file: str, verbose: bool):
        """Initialize the local LLM with configuration from a file."""
        config = load_config_file(config_file)

        super().__init__(config, verbose)

        self.model_choice = config["model_choice"]
        self.context_length = config["context_length"]
        self.max_tokens = config["max_output_tokens"]

        self.model = GPT4All(self.model_choice)

    def get_LLM_response(self, prompt: str) -> str:
        """Get a response from the local LLM for a given prompt.

        Args:
            prompt (str): The prompt to be sent to the local LLM.

        Returns:
            str: The response generated by the local LLM.
        """
        return self.model.generate(prompt, self.max_tokens)
